{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional Neural Network (CNN) is a neural network designed to process data with a similar grid structure. For example, time series data (a one-dimensional grid formed by regular sampling on the time axis) and image data (a two-dimensional pixel grid). Therefore, it is widely used in various occasions such as image recognition and speech recognition. In image recognition competitions, methods based on deep learning are almost all based on CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Overall structure\n",
    "\n",
    "In the neural network introduced earlier, all neurons in adjacent layers are connected, which is called a Fully Connected layer or Affine layer. The fully connected layer is followed by the activation function ReLU layer (or Sigmoid layer). Here, 4 layers of \"Affine-ReLU\" combinations are stacked, then the fifth layer is a fully connected layer, and finally the Softmax layer outputs the final classification result.\n",
    "\n",
    "![img](images/chapter13/fully_connected.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolution and Pooling layers have been added to CNN. The layer connection order of a typical CNN is \"Convolution-ReLU-(Pooling)\" (Pooling layer is sometimes omitted).\n",
    "\n",
    "![img](images/chapter13/CNN.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Convolutional layer\n",
    "\n",
    "## 2.1 Problems with the fully connected layer\n",
    "\n",
    "The shape of the input data is \"ignored\" by the fully connected layer. For example, when inputting image data to a fully connected layer, the 3-dimensional image data (width, height, and number of channels) needs to be flattened into 1-dimensional data. In fact, in the example of the MNIST data set used earlier, the input image is a (1, 28, 28) shape with 1 channel, 28 pixels high, and 28 pixels long, but it is arranged in 1 column in the form of 784 data Input to the fully connected layer.\n",
    "\n",
    "The three-dimensional shape of the image contains important spatial information. For example, spatially adjacent pixels have similar values, each channel of RBG is closely related, and there is no correlation between pixels that are far apart, etc. There may be essential patterns that are worth extracting hidden in the 3D shape. However, because the fully connected layer ignores the shape and treats all the input data as the same neuron (neurons of the same dimension), the information related to the shape cannot be used.\n",
    "\n",
    "The convolutional layer can keep the data shape unchanged. When the input data is an image, the convolutional layer will receive the input data in the form of 3-dimensional data, and also output to the next layer in the form of 3-dimensional data. Therefore, in CNN, data with shapes such as images can be correctly understood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In CNN, the output data of the convolutional layer is called a feature map."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Convolution operation\n",
    "\n",
    "For input data, the convolution operation slides the window of the convolution kernel (or filter) at certain intervals and applies it. Multiply the elements of the convolution kernel at each position and the corresponding elements of the input, and then sum them (sometimes this calculation is called the multiplication accumulation operation). Then, save this result to the corresponding location of the output. Repeat this process at all positions, and you can get the output of the convolution operation. If a two-dimensional data is defined as $I$ and a two-dimensional convolution kernel is defined as $K$, then  \n",
    "\n",
    "$$ S(i,j) = (I *K)(i,j) = \\sum_m \\sum_n I(i+m,j+n)K(m,n)$$\n",
    "\n",
    "Assuming that (height, width) is used to represent the shape of the data and the convolution kernel, the input data size in the following figure is (5, 5), the output data size is (3, 3), and the convolution kernel size is (3, 3) , its value is\n",
    "\n",
    "$$ \\Bigg(\n",
    "   \\begin{matrix}\n",
    "   0 & 1 & 2 \\\\\n",
    "   2 & 2 & 0 \\\\\n",
    "   0 & 1 & 2\n",
    "  \\end{matrix} \\Bigg)\n",
    "$$\n",
    "\n",
    "![img](images/chapter13/numerical_no_padding_no_strides.gif)\n",
    "\n",
    "Let $i$ be the input data size, $k$ be the convolution kernel size, and $o$ be the output data size, there is a relationship:\n",
    "\n",
    "$$ o = (i - k) + 1 $$\n",
    "\n",
    "When the number of convolutional layers increases, the spatial dimension of the network will eventually be reduced to 1×1. In this case, it is impossible for the added layers to perform meaningful convolution.\n",
    "\n",
    "In a fully connected neural network, in addition to the weight parameter, there is also a bias. In CNN, the parameters of the convolution kernel correspond to the previous weights. Moreover, there is also a bias in CNN, and the bias is added to all elements to which the convolution kernel is applied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Padding\n",
    "\n",
    "Before processing the convolutional layer, it is sometimes necessary to fill in fixed data (usually 0) around the input data. This is called padding, which is often used in convolution operations. In the figure below, padding of amplitude 1 is applied to input data of size (4, 4). By filling, the input data of size (4, 4) becomes the shape of (6, 6). Then apply a convolution kernel of size (3, 3) to generate output data of size (4, 4).\n",
    "\n",
    "![img](images/chapter13/padding.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $p$ be the filling range, it is related by\n",
    "\n",
    "$$ o = (i - k) + 2p + 1 $$\n",
    "\n",
    "In the example in the figure below, $i=5, k =4, p =2$, so $o=6$\n",
    "\n",
    "![img](images/chapter13/arbitrary_padding_no_strides.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Same Padding\n",
    "\n",
    "一类特殊的填充作为称之为 same padding，即使得输出数据尺寸等于输入数据尺寸。令卷积核尺寸 $k$ 为奇数（$k = 2n+1$），填充幅度 $p = \\lfloor \\frac{k}{2} \\rfloor= n$，则有\n",
    "A special type of padding is called same padding, which makes the output data size equal to the input data size. Let the size of the convolution kernel $k$ be an odd number （$k = 2n+1$), and the filling range $p = \\lfloor \\frac{k}{2} \\rfloor= n$, then\n",
    "\n",
    "$$ o = (i - 2n - 1)+2n+1 = i $$\n",
    "\n",
    "In the example in the figure below, $i=o=5, k = 3, p=1$\n",
    "\n",
    "![img](images/chapter13/same_padding_no_strides.gif)\n",
    "\n",
    "In this case, as long as the hardware supports it, the network can contain any number of convolutional layers, because the convolution operation does not change the structure of the next layer. However, the part close to the boundary of the input data has less influence on the output data than the middle part. This may cause a certain degree of underrepresentation of the boundary pixels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Padding\n",
    "\n",
    "Another special type of padding is called full padding, even if the convolution kernel and the data just intersect, it starts to convolve, so the padding amplitude is $p = k-1$,\n",
    "\n",
    "$$o = (i - k)+2(k-1)+1 = i+k-1$$\n",
    "\n",
    "In the example in the figure below, $i=5, k = 3, p=2$, so $o=6$\n",
    "![img](images/chapter13/full_padding_no_strides.gif)\n",
    "\n",
    "It performs enough zero padding so that each pixel has been visited exactly $k$ times in each direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Stride\n",
    "\n",
    "The position interval at which the convolution kernel is applied is called **stride**. In the previous example, the stride is all 1. If the stride is set to 2, as shown in the figure below, the interval of the window where the convolution kernel is applied becomes 2 elements.\n",
    "\n",
    "![img](images/chapter13/stride.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $i$ be the input data size, $p$ be the padding width, $k$ be the size of the convolution kernel, $s$ be the stride size, and $o$ be the output data size, then:\n",
    "\n",
    "$$ o = \\lfloor \\frac{i + 2p -k}{s} \\rfloor + 1 $$\n",
    "\n",
    "In the example in the figure below, $i=5, k=3, s=2, p=1$, so $o=3$\n",
    "\n",
    "![img](images/chapter13/padding_strides.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Convolution operation of 3D data\n",
    "\n",
    "The image is three-dimensional data, and the order is (channel, height, width) when expressed as a multidimensional array. For example, the shape of the data with the number of channels C, height H, and length W can be written as $(C, H, W)$. When performing convolution operations, in addition to the height and width directions, the channel direction needs to be processed. When there are multiple feature maps in the channel direction, the convolution operation of the input data and the convolution kernel is performed according to the channel, and the results are added to obtain the output. It should be noted that the input data and the number of channels of the convolution kernel must be the same value.\n",
    "\n",
    "![img](images/chapter13/3D_conv_0.png)\n",
    "\n",
    "Here, take the data of 3 channels as an example, the input data size is (3, 5, 5), the convolution kernel size is (3, 3, 3), the padding amplitude is 0, the stride is 1, and the output data size is ( twenty two).\n",
    "\n",
    "![img](images/chapter13/3D_conv_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The step-by-step calculation sequence is as follows：\n",
    "\n",
    "![img](images/chapter13/3D_conv_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to have the output of multiple convolution operations in the channel direction, you need to use multiple convolution kernels. As shown in the figure below, by applying $FN$ convolution kernels, $FN$ output feature maps are also generated. If these $FN$ feature maps are collected together, a data body with the shape $(FN, OH,OW)$ is obtained.\n",
    "\n",
    "![img](images/chapter13/multi_channel_conv.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further improve the convolution operation including the offset addition operation, as shown in the figure below. The shape of the output result of the convolution kernel is $(FN, OH, OW)$, and the shape of the offset is $(FN, 1, 1)$. When these two data are added together, thanks to the broadcast function of NumPy, the same offset value is added per channel.\n",
    "\n",
    "![img](images/chapter13/multi_channel_conv_with_bias.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Batch processing\n",
    "\n",
    "The previous implementation of the fully connected neural network corresponds to batch processing, and the SDG algorithm corresponding to mini-batch can be realized through batch processing. Convolution operations also support batch processing. For this, it is necessary to save the data passed between the layers as four-dimensional data, and save the data in the order of (batch_num, channel, height, width).\n",
    "\n",
    "![img](images/chapter13/batch_conv.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 Motivation for convolutional layers\n",
    "\n",
    "Convolution operations use three important ideas to help improve the deep learning system: **sparse connectivity**, **parameter sharing**, and **equivariant representations**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7.1 Sparse connectivity\n",
    "\n",
    "The fully connected layer of the traditional neural network uses matrix multiplication to establish the connection between input and output. Among them, each individual parameter in the parameter matrix describes the interaction between an input unit and an output unit. This means that every output unit interacts with every input unit. However, convolutional networks are characterized by sparse connections (also known as sparse weights). This is achieved by making the size of the core much smaller than the size of the input data.\n",
    "\n",
    "For example, if there are $m$ inputs and $n$ outputs, then the matrix multiplication of the fully connected layer requires $m×n$ parameters and the time complexity of the corresponding algorithm is $O(m×n)$. If we limit the number of connections that each output has to $k$, then the sparse connection method only needs $k×n$ parameters and a running time of $O(k×n)$.In many practical applications, you only need to keep $k$ several orders of magnitude smaller than $m$ to achieve good performance in machine learning tasks.\n",
    "\n",
    "![img](images/chapter13/sparse_connectivity.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a deep convolutional network, units in the deep layers of the network may interact indirectly with most of the input, which allows the network to efficiently describe complex interactions of multiple variables by describing only the cornerstones of sparse connections. In the figure below, the gray area highlights the $g3$ neuron and its **receptive field**.\n",
    "\n",
    "![img](images/chapter13/receptive_field.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7.2 Parameter sharing\n",
    "\n",
    "Parameter sharing refers to using the same parameters in multiple functions of a model.\n",
    "\n",
    "In traditional neural networks, when calculating the output of a layer, each element of the weight matrix is used only once, and it is never used again when it is multiplied by an element of the input.\n",
    "\n",
    "In a convolutional neural network, every element of the kernel acts on every position of the input. The parameter sharing in the convolution operation ensures that we only need to learn one parameter set instead of learning a separate parameter set for each position. Although this does not change the running time of the forward propagation (still $O(k×n)$), it significantly reduces the storage requirement of the model to $k$ parameters, and $k$ is usually many orders of magnitude smaller than $m$. Therefore, convolution is greatly superior to dense matrix multiplication in terms of storage requirements and statistical efficiency.\n",
    "\n",
    "In the upper part of the figure below, the single black arrow indicates the use of the middle element of the weight matrix in the fully connected model. This model does not use parameter sharing, so the parameters are only used once. In the convolution model, because of parameter sharing, this single parameter is used for all input positions.\n",
    "\n",
    "![img](images/chapter13/parameter_sharing.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7.3 Equivariant representations\n",
    "\n",
    "The special form of parameter sharing makes the neural network layer have the property of **equivariance** to translation. If a function satisfies the input change and the output changes this property in the same way, we say that it is equivariant. In particular, if the functions $f(x)$ and $g(x)$ satisfy $f(g(x))=g(f(x))$, we say that $f(x)$ is for transformation $ g$ is equivariant. For convolution, if $g$ is an arbitrary translation function of the input, then the convolution function is equivariant to $g$.\n",
    "\n",
    "For example, let $I$ denote the brightness function of the image in integer coordinates, and $g$ denote the transformation function of the image function such that $I'=g(I)$, where the image function $I'$ satisfies $I'(x, y )=I(x-1,y)$. This function moves each pixel in $I$ one unit to the right. If we first perform this transformation on $I$ and then perform the convolution operation, the result obtained is the same as the result obtained by first convolving $I$ and then using the translation function $g$ on the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Pooling layer\n",
    "\n",
    "The pooling function uses the overall statistical characteristics of adjacent outputs at a certain location to replace the output of the network at that location. For example, the Max Pooling function uses the maximum value in the adjacent rectangular area. Other commonly used pooling functions include the average value in adjacent rectangular areas (Average Pooling), L2 norm, and a weighted average function based on the distance from the center pixel.\n",
    "\n",
    "Generally speaking, the pooling window size will be set to the same value as the stride. The following figure shows the processing sequence when the maximum pooling of 2 × 2 is carried out by step 2.\n",
    "\n",
    "![img](images/chapter13/max_pooling.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Characteristics of pooling layer\n",
    "\n",
    "- Pooling only takes the maximum value (or average value) from the target area, so there are no parameters to learn.\n",
    "\n",
    "\n",
    "- After pooling operation, the number of channels of input data and output data will not change.\n",
    "\n",
    "\n",
    "- Robust to small position changes. No matter what kind of pooling function is used, when the input makes a small shift, most of the output after the pooling function will not change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Implementation of Convolutional Layer and Pooling Layer\n",
    "\n",
    "## 4.1 Implementation of Convolutional Layer\n",
    "\n",
    "If the convolution operation is implemented normally, multiple layers of for loop statements need to be repeated, but there is a disadvantage that the use of for statements to process NumPy arrays is inefficient. Here, we do not use the for statement to implement the convolution operation, but use the convenient function im2col for simple implementation.\n",
    "\n",
    "The **im2col** (image to column) function expands the input data to fit the convolution kernel (weight). After applying im2col to the three-dimensional input data, the data is converted into a two-dimensional matrix, as shown in the figure below.\n",
    "\n",
    "![img](images/chapter13/im2col_3D.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After using im2col to expand the input data, you only need to expand the convolution kernel (weight) of the convolution layer into 1 column vertically, and calculate the product of these two matrices.\n",
    "\n",
    "![img](images/chapter13/trans_kernel.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further consider the batch size, so that the shape of the input data is $(N, C, H, W)$, the shape of the convolution kernel is $(FN, C, FH, FW)$, and the shape of the output data is $(N, FN, OH, OW)$. The shape of the two-dimensional data returned by the im2col function is $(N \\times OH \\times OW, C \\times FH \\times FW)$, and the convolution kernel is expanded into a two-dimensional array of shape $(C \\times FH \\times FW, FN)$, then, matrix multiplication is performed, and finally convert the two-dimensional output data into a suitable shape. The entire convolution operation process is shown in the figure below,\n",
    "\n",
    "![img](images/chapter13/im2col_4D.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def im2col(input_data, filter_h, filter_w, stride=1, pad=0):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_data : Input data composed of a four-dimensional array of (number, channel, height, width)\n",
    "    filter_h : Convolution kernel height\n",
    "    filter_w : Convolution kernel length\n",
    "    stride : Stride\n",
    "    pad : Padding\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    col : 2-dimensional array\n",
    "    \"\"\"\n",
    "    N, C, H, W = input_data.shape\n",
    "    out_h = (H + 2*pad - filter_h)//stride + 1\n",
    "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
    "\n",
    "    img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant')\n",
    "    col = np.zeros((N, C, out_h, out_w, filter_h, filter_w))\n",
    "\n",
    "    for y in range(filter_h):\n",
    "        y_max = y + stride*out_h\n",
    "        for x in range(filter_w):\n",
    "            x_max = x + stride*out_w\n",
    "            col[:, :, :, :, y, x] = img[:, :, y:y_max:stride, x:x_max:stride]\n",
    "\n",
    "    col = col.transpose(0, 2, 3, 1, 4, 5).reshape(N*out_h*out_w, -1)\n",
    "    return col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case where the application areas of the convolution kernel overlap, after using the im2col function to expand, the number of elements after expansion will be more than the number of elements in the original square. Therefore, the implementation using im2col has the disadvantage of consuming more memory than the ordinary implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, use im2col to implement the convolutional layer. Here we implement the convolutional layer as a class named Convolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convolution:\n",
    "    def __init__(self, W, b, stride=1, pad=0):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "\n",
    "    def forward(self, x):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = (H + 2*self.pad - FH)// self.stride + 1\n",
    "        out_w = (W + 2*self.pad - FW)// self.stride + 1\n",
    "\n",
    "        col = im2col(x, FH, FW, self.stride, self.pad)\n",
    "        col_W = self.W.reshape(FN, -1).T # Expand the convolution kernel\n",
    "        out = np.dot(col, col_W) + self.b\n",
    "\n",
    "        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the implementation of the forward() function, the output data will be converted into a suitable shape at the end.\n",
    "\n",
    "![img](images/chapter13/transpose.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is the implementation of the forward processing of the convolutional layer. After data expansion using im2col, it can be implemented like a fully connected layer. When performing the back propagation of the convolutional layer, the inverse processing of im2col must be performed. See the code in the demo_code/layers.py file for specific implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Implementation of the pooling layer\n",
    "\n",
    "The implementation of the pooling layer is the same as the convolutional layer, and both use im2col to expand the input data. However, in the case of pooling, it is independent in the channel direction, so the application area of pooling is expanded separately by channel.\n",
    "\n",
    "![img](images/chapter13/pooling.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After expanding like this, you only need to find the maximum value of each row of the expanded matrix and convert it to a suitable shape.\n",
    "\n",
    "![img](images/chapter13/pooling_implementation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The realization of the pooling layer is carried out in the following three stages:\n",
    "- Expand input data\n",
    "- Find the maximum value of each row\n",
    "- Convert to a suitable output size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pooling:\n",
    "    def __init__(self, pool_h, pool_w, stride=1, pad=0):\n",
    "        self.pool_h = pool_h\n",
    "        self.pool_w = pool_w\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.shape\n",
    "        \n",
    "        # Expansion\n",
    "        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        col = col.reshape(-1, self.pool_h*self.pool_w)\n",
    "        \n",
    "        # Maximum\n",
    "        out = np.max(col, axis=1)\n",
    "        \n",
    "        # Conversion\n",
    "        out_h = (H - self.pool_h) // self.stride + 1\n",
    "        out_w = (W - self.pool_w) // self.stride + 1\n",
    "        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The backward processing of the pooling layer can refer to the reverse propagation of max used in the implementation of the ReLU layer. For the specific implementation, see the code in the demo_code/layers.py file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Implementation of CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have implemented the convolutional layer and the pooling layer, now we will combine these layers to build a convolutional neural network for handwritten digit recognition. The composition of the network is \"Convolution-ReLU-Pooling-Affine-ReLU-Affine-Softmax\", and we implement it as a class called SimpleConvNet.\n",
    "\n",
    "![img](images/chapter13/CNN_mnist.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters of the initialization function (\\_\\_init\\_\\_) of SimpleConvNet are as follows:\n",
    "- input_dim ― dimensions of the input data: (channel, height, length)\n",
    "- conv_param ― the hyperparameters (dictionaries) of the convolutional layer. The keywords of the dictionary are as follows:\n",
    "      filter_num ― Number of convolution kernels\n",
    "      filter_size ― The size of the convolution kernel\n",
    "      stride ― Stride\n",
    "      pad ― Padding\n",
    "- hidden_size ― Number of neurons in the hidden layer (fully connected)\n",
    "- output_size ― Number of neurons in the output layer (fully connected)\n",
    "- weitght_int_std ― Standard deviation of weights at initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleConvNet:\n",
    "    \"\"\"Simple ConvNet\n",
    "\n",
    "    conv - relu - pool - affine - relu - affine - softmax\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_size : Input size (784 in the case of MNIST)\n",
    "    hidden_size_list : List of the number of neurons in the hidden layers (e.g. [100, 100, 100])\n",
    "    output_size : Output size (10 in the case of MNIST)\n",
    "    activation : 'relu' or 'sigmoid'\n",
    "    weight_init_std : Specify the standard deviation of the weight (e.g. 0.01)\n",
    "        Set \"Initial value of He\" when specifying 'relu' or'he'\n",
    "        Set \"Xavier initial value\" when specifying'sigmoid' or 'xavier'\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=(1, 28, 28), \n",
    "                 conv_param={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1},\n",
    "                 hidden_size=100, output_size=10, weight_init_std=0.01):\n",
    "        filter_num = conv_param['filter_num']\n",
    "        filter_size = conv_param['filter_size']\n",
    "        filter_pad = conv_param['pad']\n",
    "        filter_stride = conv_param['stride']\n",
    "        input_size = input_dim[1]\n",
    "        conv_output_size = (input_size - filter_size + 2*filter_pad) / filter_stride + 1\n",
    "        pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))\n",
    "\n",
    "        # Weight initializarion\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
    "        self.params['b1'] = np.zeros(filter_num)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(pool_output_size, hidden_size)\n",
    "        self.params['b2'] = np.zeros(hidden_size)\n",
    "        self.params['W3'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b3'] = np.zeros(output_size)\n",
    "\n",
    "        # Layers generation\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'],\n",
    "                                           conv_param['stride'], conv_param['pad'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
    "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.layers['Relu2'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
    "\n",
    "        self.last_layer = SoftmaxWithLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters required for learning are the weights and biases of the first layer of convolutional layer and the remaining two fully connected layers. These parameters are saved in the params dictionary of the instance variable after initialization.\n",
    "\n",
    "Then, add layers to the layers of the OrderedDict in order from the top. Only the last SoftmaxWithLoss layer is added to the other variable last_layer.\n",
    "\n",
    "After initialization like this, the predict method for inference and the loss method for finding the value of the loss function can be implemented as follows.\n",
    "\n",
    "```python\n",
    "def predict(self, x):\n",
    "    for layer in self.layers.values():\n",
    "        x = layer.forward(x)\n",
    "    return x\n",
    "\n",
    "def loss(self, x, t):\n",
    "    y = self.predict(x)\n",
    "    return self.last_layer.forward(y, t)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next is the code implementation of obtaining the gradient based on the error back propagation method. The gradient of the parameters is obtained by the error back propagation algorithm, because the functions of forward propagation and back propagation have been correctly implemented in each layer, so it only needs to be called in the appropriate order here. Finally, save the gradient of each weight parameter to the grads dictionary.\n",
    "\n",
    "```python\n",
    "def gradient(self, x, t):\n",
    "    # forward\n",
    "    self.loss(x, t)\n",
    "\n",
    "    # backward\n",
    "    dout = 1\n",
    "    dout = self.last_layer.backward(dout)\n",
    "\n",
    "    layers = list(self.layers.values())\n",
    "    layers.reverse()\n",
    "    for layer in layers:\n",
    "        dout = layer.backward(dout)\n",
    "\n",
    "    # set up\n",
    "    grads = {}\n",
    "    grads['W1'], grads['b1'] = self.layers['Conv1'].dW, self.layers['Conv1'].db\n",
    "    grads['W2'], grads['b2'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "    grads['W3'], grads['b3'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "\n",
    "    return grads\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, use this SimpleConvNet network to train on the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:1, train acc:0.275, test acc:0.223 ===\n",
      "=== epoch:2, train acc:0.816, test acc:0.806 ===\n",
      "=== epoch:3, train acc:0.883, test acc:0.871 ===\n",
      "=== epoch:4, train acc:0.902, test acc:0.888 ===\n",
      "=== epoch:5, train acc:0.922, test acc:0.921 ===\n",
      "=== epoch:6, train acc:0.928, test acc:0.924 ===\n",
      "=== epoch:7, train acc:0.948, test acc:0.914 ===\n",
      "=== epoch:8, train acc:0.95, test acc:0.941 ===\n",
      "=== epoch:9, train acc:0.962, test acc:0.944 ===\n",
      "=== epoch:10, train acc:0.965, test acc:0.955 ===\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.941\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import mnist\n",
    "from demo_code.simple_convnet import SimpleConvNet\n",
    "from demo_code.trainer import Trainer\n",
    "\n",
    "max_epochs = 10\n",
    "\n",
    "# Read data\n",
    "(x_train, t_train), (x_test, t_test) = mnist.load_data()\n",
    "x_train = x_train.astype(np.float32) / 255.0\n",
    "x_test = x_test.astype(np.float32) / 255.0\n",
    "\n",
    "x_train = x_train[:, np.newaxis, :, :]\n",
    "x_test = x_test[:, np.newaxis, :, :]\n",
    "\n",
    "# Reduce data when processing takes a long time\n",
    "x_train, t_train = x_train[:6000], t_train[:6000]\n",
    "x_test, t_test = x_test[:1000], t_test[:1000]\n",
    "\n",
    "network = SimpleConvNet(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "\n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you use the MNIST data set to train the SimpleConvNet network, the recognition rate of the training data is 99.82%, and the recognition rate of the test data is about 98.96% (some errors will occur in the recognition accuracy of each learning). The recognition rate of the test data is about 99%, which is a very high recognition rate for small networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. CNN visualization\n",
    "\n",
    "We just did a simple CNN learning on the MNIST data set. At that time, the shape of the weight of the convolutional layer of the first layer was (30, 1, 5, 5), that is, 30 convolution kernels with a size of 5 × 5 and a channel of 1. This means that the convolution kernel can be visualized as a single-channel 5 × 5 grayscale image. Now, we display the convolution kernel of the convolution layer (layer 1) as an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 30 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcEAAACZCAYAAABXEYHGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAWY0lEQVR4nO3deWxU5fcG8NPOTGdKp8t02lK2UkBscUFERIwLICKCotGoaMToPxA2TRRcIglGjEvEBYLBuECNwQUwcY+CggZxKy3UskPZWrBKWyrF7tPe7x/+bDqXvs9hxF+0vs/nL4bnvXdub+fO6TT39MQ5jiNEREQ2iv+nD4CIiOifwiJIRETWYhEkIiJrsQgSEZG1WASJiMhaLIJERGQtbyyLk5OTnczMTGOelJQEt29qaoL5kSNHzmh7x3Hi/vy3z+dzAoGAca3P54P7am9vh3lcXBzMte3r6uqqHcfpOJlpaWlOdna2cb12brVWF+3ctba2wrysrKzjeAOBgBMMBo1ra2pq4L7S09Nh3qtXL5ij76uISHFxcdS5DQaDTjgcNq7/9ddf4f60c5uWlgbzUCgE8z179kQdb48ePRy0z7a2Nri/5uZmmJ/GdWTMIpGItLW1dbz4MzIynJycHOP6+Hj8c3ZdXR3MKysrYd7Q0ADz9vb2mF4L2mvX68VvmR6PB+bo/VMk+rXg8Xgc9HwtLS1wXykpKTD3+/0wT0xMhHl5eXnUuc3IyHByc3ON63/55Re4P+06096jtPfkY8eORR3vn2IqgpmZmbJw4UJjPmrUKLj97t27Yf7ggw+e0fadBQIBGT58uDFHBUdEpL6+HuYJCQkw1y7OtWvXHnYfz6uvvmpcr53bSCQC8507d8Jce4FOnjy543iDwaDccMMNxrUFBQVwX5MmTYL5/PnzYT548GCYe73eqHMbDofl4YcfNq5fsmQJ3J928V1//fUwv+WWW2A+evToqONNS0uTadOmGdefOHEC7u/gwYMw37VrF8xREXW/TnJycmTTpk3G9T169IDPtW7dOpg/+eSTMC8uLoZ5fX39Ka+FRx991Lj+jTfegPvTihj64VBEZM6cOTC/7LLLOo7X6/VK7969jWsPHTqk7QvmqGCJiAwdOhTmM2fOjDq3ubm5UlRUZFy/aNEiuD/th7eff/4Z5toPKEuXLj3c1f/z16FERGQtFkEiIrIWiyAREVmLRZCIiKzFIkhERNaK6e5Qn88H71bq2bMn3F67K62xsRHm6O4f992Rfr9fBg4caFyv3f6r3WGXn58P82uuuQbma9eujXp84sQJ+fzzz43rBwwYAPeHvi8iIoMGDYL5iBEjYN6Z1+uFd8nNmDEDbj9v3jyYa8f6ySefwNytvr5eCgsLjbl2Z+yNN94Ic3SnrIhIXl4ezN3q6+tl8+bNxlw73sOHu7wJroPWvoOum+rq6lP2he6ELikpgc+1atUqmG/btg3m2nuGm9/vh3dFatctaq8QwedORL8js7OEhATp37+/Mde+z9p1dMUVV8Bce49027t3r4wbN86YV1RUwO2vuuoqmM+ePRvmycnJMF+6dGmX/89PgkREZC0WQSIishaLIBERWYtFkIiIrMUiSERE1mIRJCIia7EIEhGRtWLqE/T7/XL22Wcbc61PQxv9oeVoVIY7a21thSNytCkP2rgfbbSR1hPp9ttvv8n7779vzLUJGlovmjbu58ILL4R5Z6FQSG6++WZjrvWhaef2xx9/hLn21+TdsrOz4RQJrUdSO15tqoOWuzU3N0tZWZkx37dvH9w+KysL5lrPKZqK8fLLL5/yf2gEjjbpQJtuUltbC3PtOnRPg0lJSZEJEyYY12vnduTIkTDXeqW1ft7O8vLy5OuvvzbmK1asgNtrI8L2798P8379+sHcze/3w95ErYdSm8zz3HPPwVzrizThJ0EiIrIWiyAREVmLRZCIiKzFIkhERNZiESQiImuxCBIRkbVYBImIyFoxzxNEc+QqKyvh9loPDuo3EhHp1auXMXPPWEtMTJTzzz/fuN49f9BN6yNcv349zLWv1a2pqQn2AqK+MRGRDz/8EOYZGRkwv+iii2DeWXx8vAQCAWO+detWuL3W56fNmIt1zlkgEIDzH5ctWwa3Ly4uhnlzczPMY50nqM3oQ69rEb3vccyYMX95+3fffTfqcSQSkaqqKuN6bR5grD2fbtrr2t0neOLECfnss8+M64uKiuD+tH7d0tJSmI8fPx7mnTU0NMiWLVuM+RdffAG312Yfattr8/3c6uvrYY9vamoq3F7rL9bqi3buTfhJkIiIrMUiSERE1mIRJCIia7EIEhGRtVgEiYjIWiyCRERkLRZBIiKyVkx9giK4l6+xsRFuW15eDnNtHiHav/u4evbsKffdd59xfZ8+feBzLV++HOaFhYUw9/v9MHf3lnk8HklJSTGu//333+H+fD4fzLWZdlpvX2daH5s2r047dyUlJTD3eDww7wqaRanNTaupqYE56uUSEXnnnXdg7paamiqTJk0y5ldeeSXcfurUqTDXrsOjR48as7a2tqjHDQ0NsD8LzcMTEamoqIC59rpG39euOI4D+zrRNSiCz42IyDnnnAPzBQsWwLyzY8eOyZIlS4y5dh2sW7cO5keOHIH5wYMHYe6WnJwsY8eONeYTJ06E2xcUFMD822+/hbk2W9KEnwSJiMhaLIJERGQtFkEiIrIWiyAREVmLRZCIiKzFIkhERNZiESQiImvFaTP8ohbHxVWJyOH/v8M5I/0dx+kYdvgvP1aRbny83elYRXi8f7PudKwi3fh4u9OxinS/4/1TTEWQiIjov4S/DiUiImuxCBIRkbVYBImIyFosgkREZC0WQSIishaLIBERWYtFkIiIrBXTUN1wOOygAaStra1we2046fHjx2GemJhozBobG6WlpaVjwmZCQoITCASM6+vr6+Fztbe3w1wb5un14lPb2tpa3blx0+PxOGiAKBoEKiISDAZhnpqaCnN0rkRE9u/f33G8GRkZTk5OjnFtfDz+2Uobvnzs2DGYo4G+/5dHnVufz+egIcfuQbFusQ5uddMGw9bV1UUdbzgchuc3EonA/VVVVcFcu85Q73BbW5u0t7d3nJCkpCQnFAoZ12vDoFtaWmCuDbnVtq+trY06tykpKU5WVpZxvXYdnOl7nPa+09TUVN2pWR42cWtDyHv06HFGx6L5/fffT7nO0PnT3lO1objp6ekwT0hIgPm2bduqu2qWj6kI9uvXT9avX2/MtanLK1euPKN82LBhxsw9dTgQCMgll1xiXP/DDz/A59IuXq3IZWRkwLyysjLqLyv4fD7Jzc01rt+zZw/c3/Dhw2GOJpWLiOTl5cH8pptu6jjenJwcOOUZ/bAiIvLTTz/B/KWXXoK5Nnm+qKgo6tz6/X45//zzjet/++03uD/tjVEror1794b52rVro443JydHvvrqK+P62tpauD/t/K1evRrmqLC4C2goFJLZs2cb12vXmTa9/Nprr4V5eXk5zFetWhV1brOysuSFF14wrteuA20a+1tvvQXz77//Hua7d+8+7b+4MmLECJhffPHFMC8sLIS5VrQ2btwYdayBQAC+R2s/vI4cORLmd911F8zRBzQRkZycnC7PLX8dSkRE1mIRJCIia7EIEhGRtVgEiYjIWiyCRERkrZjuDvV6vRIOh425dkdlWVkZzLVb6ydMmGDMtm/fHvU4NTUVrh88eDB8rqamJphr0tLSYP7iiy9GPY6Li4O30mt3Rg0cOBDm6O5IEZE+ffrAvLP29nbY5vDss8/C7deuXQvzAwcOwPzss8+GuVtSUhK8Uxjd6Sryxx2QiHZ3aP/+/WHu5vF44OuntLQUbq/d9addp+juXne7SHt7O7zrT7sz9vbbb4e59lrR7h5dtWpV1OO0tDS54YYbjOu/+eYbuL/nn38e5rt27YJ5LEKhkIwbN86Yn3XWWXD7++67D+Zz586F+dixY2G+cePGqMeRSATeuazdha21gWnvYVrLiAk/CRIRkbVYBImIyFosgkREZC0WQSIishaLIBERWYtFkIiIrMUiSERE1oqpT7CxsVG2bdtmzO+55x64vdaf1LNnT5ijURoejyfqcTAYlCuuuMK4fsaMGfC5tNFEe/fuhbn21/HdfYJ9+/aF/XVoqoCIyHvvvQdzzZAhQ0577YEDB2TKlCnGfMOGDXB7bSSK9jrQjtXd61VTUyNvvvmmcb3Wv4Sme4iIVFZWwvzcc8+FeVfQX/DXXgsVFRUw175e1KPo7hNsbm6W/fv3w/0hy5Ytg3mvXr1gjnqBu1JdXS2vv/66MX/ttdfg9toEFG1kmfZaOHToUMe/09PT5c477zSuzc7Ohvuqrq6GuTYdZdq0aTCfPn161OO2tjapq6szrtdGpGn1QevH/au93fwkSERE1mIRJCIia7EIEhGRtVgEiYjIWiyCRERkLRZBIiKyFosgERFZK6Y+wQMHDsgdd9xhzHfs2AG3z8/Ph3lmZibMW1pajJnjOFGPtX6gfv36wedKSEiA+fHjx2GuzcZy8/l8kpWVZcw3b94Mt9d6vxYvXgzzyy+/HOadnTx5Ur788ktjPmrUKLi91t+E5iqK/DG3LBZtbW1wzpnWl6j1L40YMQLmsfYv1dfXw++39lrQXpvarEs0a7OmpibqcWZmpsyaNcu4Xpt9+Nhjj8H86NGjMHf322qOHj0qjz76qDGvqqqC22sz/Pr27QvzWOZ2BgIB+J750ksvwe3XrFkD8/POOw/mN910E8zd4uPjYe9ha2sr3L5zj2RXtNmS2rxBE34SJCIia7EIEhGRtVgEiYjIWiyCRERkLRZBIiKyFosgERFZi0WQiIisFVOfYFpamkyePNmYP/3003D7w4cPw3zPnj0wD4VCxszrjf5SampqZOXKlXB/SGJiIsy13jLU89eVHj16yEUXXWTMn3rqKbh9QUEBzDdt2gRz9ww+JDExEfYvDRgwAG4fDodh3tDQcNrHcjpCoZCMHz/emKP+UxH967ngggtgfvfdd8PcPcPu2LFjsAesuLgY7k/ra9R63dBr193DGQwGYY+pu3/XTetF0+b7zZkzB+aLFi2KeqzNvBs2bBjcnzbLUuvX1WbidVZXVwd740aOHAm3RzMpRfQezE8//RTmbj6fD/YA//rrr3B7bS7nu+++C/N9+/bB3ISfBImIyFosgkREZC0WQSIishaLIBERWYtFkIiIrMUiSERE1mIRJCIia8VpfTxRi+PiqkQEN/v9c/o7jtMxkPBffqwi3fh4u9OxivB4/2bd6VhFuvHxdqdjFel+x/unmIogERHRfwl/HUpERNZiESQiImuxCBIRkbVYBImIyFosgkREZC0WQSIishaLIBERWSumobrx8fGOx+Mx5lrPYVxcHMy1IZBa7jhOxxMEg0EnPT3duLa5uRnuq76+Huba9gkJCTBvaGio7ty4mZqa6pzJQErt3KDv2+nkNTU1HcebnJzsoMG42vDkQCAA88zMU/pZoyQnJ8N8586dUec2FAo5ffr0Ma5vbGyE+9OG7mrnXtt/bW1ttavpGF5I2sBn9DoSOXUAtVskEjFmVVVVcvLkyY7rLBAIOOj7kZqaCp8rLS0N5hptSG1JSUnUuQ0EAk5SUpJxvTYUV7uug8EgzP1+P8yPHj162teZ9n6rvS6PHz8Oc+17V1lZGXVuMzIynNzc3L98PNp1or1Htba2wnzv3r3VXTXLx1QEPR6PZGRkGHOtMGhfhHYSUO4+wenp6fLggw8a15eVlcHnKiwshPn+/fthrk0jLywsjKoU2dnZ8sorrxjXL168GO4PTcsW0V/QoVAI5gUFBR3HGw6HZf78+ca106dPh/vSzs2sWbNgPnr0aJgPHTo06tz26dNHVq9ebVy/c+dOuL8jR47AXJvkXlpaCvM1a9bE9Fc28vLyYD537lyYo8nxIn9MtjdZsGBB1OPk5GQ4Hf66666Dz3XjjTfCXCty2g+rqampUec2KSlJJk6caFz/wQcfwP3l5OTA/LLLLoP5WWedBfNHHnkk6jpzn+/O0A8rIvq50Sa1o/MkIvL4449Hndvc3FwpKir6y8ejXSfoBwIR/TodN25cl9cZfx1KRETWYhEkIiJrsQgSEZG1WASJiMhaLIJERGStmO4O1TQ0NMBcu9NLy2MZ+9TY2Cjbt2835uvWrYPbHzp0COao/UJE5LbbboO5++7Tn3/+WR577DHj+hMnTsD9XXvttTBHd/WKiOTn58O8oKCg498ejwfebTp+/Hi4L+1W5uHDh8Nca1lwi0Qi8HZw7e5Q7XuttWxod9mtWbMm6nG/fv3gnc2DBw+G+9POf3FxMcx37NhhzNzfu4aGBvnpp5+M67U7GCsrK2Het29fmGt3ebs1NjbCuxDj4/HnAu096Oqrr4b5hAkTYP7II4+c9vNp77dVVVUwf+aZZ2Cu3cX9+OOPRz2urq6W5cuXG9drLRlDhgyBOXo/F9Fb8Ez4SZCIiKzFIkhERNZiESQiImuxCBIRkbVYBImIyFosgkREZC0WQSIislZMfYLJyckyZswYY/7LL7/A7bUpE1pP0cmTJ42Zu6/v5MmTsmHDBuN6bdyPNs7n/vvvh/kDDzwA83nz5kU9DofDMnXqVOP6gwcPwv1pf91em+Ch9Z51FolE4MgZbXKANgooJSUF5rH0i4r8MR7no48+MubaaCGtp1TrT1qyZAnM3bKysuTee+815q+++irc/oknnoC51teIpmK4e3kjkYhUV1cb12/atAk+15YtW2Cu9Q7H2jM6aNAgef/99425u2fTTTterZdNG4nWWUtLi5SXlxvzrVu3wu21XuWLL74Y5lr/rFt5ebnMnDnTmKMRViIit956K8y177XW42nc7i9tRURE9B/AIkhERNZiESQiImuxCBIRkbVYBImIyFosgkREZC0WQSIislZMfYK5ublwXlRFRQXcvra2FuaBQADm7hl8nT355JNRj1tbW+GsMq2vbsqUKTBHfVwiIjU1NTB3S09Ph32CWt+hNletpKQE5p988gnMO0tNTYXzC4uKiuD2K1euhHnn2YVd0fr6ulqP5ilqPZjabEmtL/Ltt9+GuVtFRQXsQ9X6cdPS0mAeDodhjua+ufv2AoEA7DHV+ua0eYKoZ1FEn3nn5vf7ZdCgQcZc61XWzr3WZ6j1H7uhntj29na47e7du2G+aNEimGtfq5vX65Xs7GxjnpubC7f/+OOPz+h4Lr30Upib8JMgERFZi0WQiIisxSJIRETWYhEkIiJrsQgSEZG1WASJiMhaLIJERGSt2BquBPet5OXlwW212WB1dXUwR30x7llVfr8f9i8NHToUPtfkyZNhHgwGYa715bnFx8fDOXsTJ06E25eWlsJcm+uG5u25JSQkwD7LL7/8Em6v9VBqsx61Pje37Oxseeihh4y51r+6fv16mGvn9plnnoG5W3JysowdO9aYa9dRfn4+zLXevRUrVhgz90xPr9cLe9+0OZU+nw/mTU1NMNdmT7p7QLU5o36/H+6vb9++MEffNxGR7777Duad9e7dWxYuXGjMtd7effv2wVzrj9XmVrr16tVLHn74YWOuvefu2rUL5suWLYO5Vj9M+EmQiIisxSJIRETWYhEkIiJrsQgSEZG1WASJiMhaLIJERGQtFkEiIrJWHOr7O2VxXFyViOAmrn9Of8dxOhqW/uXHKtKNj7c7HasIj/dv1p2OVaQbH293OlaR7ne8f4qpCBIREf2X8NehRERkLRZBIiKyFosgERFZi0WQiIisxSJIRETWYhEkIiJrsQgSEZG1WASJiMhaLIJERGSt/wEamSc3Tl5JGAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 30 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def filter_show(filters, nx=10):\n",
    "    FN, C, FH, FW = filters.shape\n",
    "    ny = int(np.ceil(FN / nx))\n",
    "\n",
    "    fig = plt.figure()\n",
    "    fig.subplots_adjust(left=0, right=1, bottom=0, top=0.5, hspace=0.0, wspace=0.1)\n",
    "\n",
    "    for i in range(FN):\n",
    "        ax = fig.add_subplot(ny, nx, i+1, xticks=[], yticks=[])\n",
    "        ax.imshow(filters[i, 0], cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "network = SimpleConvNet()\n",
    "# Weight after random initialization\n",
    "filter_show(network.params['W1'])\n",
    "\n",
    "# Weight after learning\n",
    "network.load_params(\"demo_code/params.pkl\")\n",
    "filter_show(network.params['W1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen from the figure above, the filter before learning is initialized randomly, and there is no rule to follow. However, the learned filter becomes a regular image. Some filters respond to edges in the vertical direction, and some filters respond to edges in the horizontal direction.\n",
    "\n",
    "The above results are for the first convolutional layer. The convolutional layer of the first layer extracts \"low-level\" information such as edges or patches. In a stacked CNN, what kind of information will be extracted in each layer? According to research related to visualization of deep learning, as the level deepens, the extracted information becomes more and more abstract.\n",
    "\n",
    "The figure below shows an 8-layer CNN for general object recognition. The name of this network structure is AlexNet which will be introduced in the next section. The AlexNet network structure stacks multiple convolutional layers and pooling layers, and finally outputs the result through a fully connected layer.\n",
    "\n",
    "![img](images/chapter13/CNN_visual.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the convolutional neural network deepens, the extracted information becomes more complex and abstract. The first layer responds to simple edges, the next layers respond to textures, and the later layers respond to more complex object parts. In other words, as the level deepens, neurons change from simple shapes to \"high-level\" information."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
